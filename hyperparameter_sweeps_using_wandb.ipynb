{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter-sweeps-using-wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP5QgP1ytBi2gDKutfSnz8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/effective-ml-mini-projects/blob/main/hyperparameter_sweeps_using_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-uLCK23-KIs"
      },
      "source": [
        "# How to do a Hyperparameter Sweep in Weights and Biases\n",
        "\n",
        "A hyperparameter sweep is used when we want to do hyperparameter tuning of our machine learning model. Weights and Biases allows uses to do sweeps fairly easily and creates some beautiful graphs to see how well our model did after training it with different sets of hyperparameters.\n",
        "\n",
        "# An Overview of Sweeps\n",
        "\n",
        "There a 3 steps to running a sweep with Weights and Biases:\n",
        "\n",
        "1. **Define the sweep:** create a dictionary or YAML file that specifies the parameters to search through, the search strategy, the optimization strategy, etc.\n",
        "\n",
        "2. **Initialize the sweep:** initialize the sweep and pass the dictionary of sweep configurations with: `sweep_id = wandb.sweep(sweep_config)`.\n",
        "\n",
        "3. **Run the sweep agent:** call `wandb.agent(sweep_id, function=train)`, where `function` defines the model architecture and trains it.\n",
        "\n",
        "If you decide the create a YAML file for your sweep, it should look something like this for a **deep learning** model:\n",
        "\n",
        "    # sweep.yaml\n",
        "    program: train.py\n",
        "    method: random\n",
        "    metric:\n",
        "     name: val_loss\n",
        "     goal: minimize\n",
        "    parameters:\n",
        "     learning-rate:\n",
        "       min: 0.00001\n",
        "       max: 0.1\n",
        "     optimizer:\n",
        "       values: [\"adam\", \"sgd\"]\n",
        "     hidden_layer_size:\n",
        "       values: [96, 128, 148]\n",
        "     epochs:\n",
        "       value: 27\n",
        "    early_terminate:\n",
        "       type: hyperband\n",
        "       s: 2\n",
        "       eta: 3\n",
        "       max_iter: 27\n",
        "\n",
        "And if you use a python dictionary, it should look like this for an xgboost model:\n",
        "\n",
        "    sweep_config = {\n",
        "        \"method\": \"random\", # try grid or random\n",
        "        \"metric\": {\n",
        "          \"name\": \"accuracy\",\n",
        "          \"goal\": \"maximize\"   \n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"booster\": {\n",
        "                \"values\": [\"gbtree\",\"gblinear\"]\n",
        "            },\n",
        "            \"max_depth\": {\n",
        "                \"values\": [3, 6, 9, 12]\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"values\": [0.1, 0.05, 0.2]\n",
        "            },\n",
        "            \"subsample\": {\n",
        "                \"values\": [1, 0.5, 0.3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "If you want to run a sweep from the command-line, you can run the following commands:\n",
        "\n",
        "1. **Setup a new sweep:** `wandb sweep sweep.yaml` which creates your sweep, and returns both a unique identifier (SWEEP_ID) and a URL to track all your runs.\n",
        "\n",
        "2. **Launch the sweep:** `wandb agent SWEEP_ID`, this will start the hyperparameter sweep and return the URL where you can track the sweep's progress. You can also launch multiple agents (GPUs / CPUs) concurrently. Each of these agents will fetch parameters from the W&B server and use them to train the next model.\n",
        "\n",
        "Documentation on sweeps can be found here: https://docs.wandb.ai/guides/sweeps/quickstart\n",
        "\n",
        "**Tips:** \n",
        "\n",
        "1. You are probably going to end up doing more than one hyperparameter sweep, so start out broad and then hone in on the hyperparameter space with the best performance for your next sweeps.\n",
        "\n",
        "2. Try to use log ditributed sweeps (especially for batch size, learning rate, and hidden layer size). Instead of doing sweeps uniformly between every value in 1 to 1000, you can try every order of magnitude (1, 10, 100, 1000). For example, `q_log_uniform` will try different orders of magnitude with equal probability.\n",
        "\n",
        "Now, let's get started with code!\n",
        "\n",
        "## Setup\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E9tAzvxY6cKu",
        "outputId": "9f356aec-9306-4520-d181-4ee516228a31"
      },
      "source": [
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 266 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 276 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 286 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 296 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 317 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 327 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 337 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 348 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 368 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 378 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 389 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 399 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 419 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 430 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 440 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 450 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 471 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 481 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 491 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 501 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 522 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 532 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 542 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 552 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 573 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 583 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 593 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 604 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 614 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 624 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 634 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 645 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 655 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 665 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 675 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 686 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 696 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 706 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 716 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 727 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 737 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 747 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 757 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 768 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 778 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 788 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 798 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 808 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 819 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 829 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 839 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 849 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 860 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 870 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 880 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 890 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 901 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 911 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 921 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 931 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 942 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 952 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 962 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 972 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 983 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 993 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=580db028ae93221d0570b1fc85539b87a47285d8cb816f2155e964129ecd1a22\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a22759c95f1c2e25fe913fb615a6019d893a973a69d0873ce28817f19d3f7c69\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSFtaCRrJoW0"
      },
      "source": [
        "# Step 1. Define the Sweep\n",
        "\n",
        "Fundamentally, a Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evaluates them. We configure it with different hyperparameters and a method like bayesian optimization (`bayes`) or `random` search.\n",
        "\n",
        "Usually we choose either random search or bayesian search. Bayesian is great, but it does not do so well when you have a ton of hyperparameters (scales poorly as a function of number of hyperparameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOGh-eOKJhBB"
      },
      "source": [
        "import math\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    },\n",
        "    'parameters': {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "    },\n",
        "    'fc_layer_size': {\n",
        "        'values': [128, 256, 512]\n",
        "    },\n",
        "    'dropout': {\n",
        "        'values': [0.3, 0.4, 0.5]\n",
        "    },\n",
        "    'epochs': {\n",
        "        'value': 1\n",
        "    },\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 0 and 0.1\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0,\n",
        "        'max': 0.1\n",
        "    },\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms\n",
        "        'distribution': 'q_log_uniform', # Quantized log uniform. Returns round(X, q) \n",
        "                                         # allows you to try every order of magnitude uniformly\n",
        "        'q': 1,\n",
        "        'min': math.log(32),\n",
        "        'max': math.log(256),\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# We can also include hyperband for early stopping"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znO2-TyhX0MB",
        "outputId": "10931a51-11eb-48f1-9b39-8be3a7ade20c"
      },
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'random',\n",
            " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
            " 'parameters': {'batch_size': {'distribution': 'q_log_uniform',\n",
            "                               'max': 5.545177444479562,\n",
            "                               'min': 3.4657359027997265,\n",
            "                               'q': 1},\n",
            "                'dropout': {'values': [0.3, 0.4, 0.5]},\n",
            "                'epochs': {'value': 1},\n",
            "                'fc_layer_size': {'values': [128, 256, 512]},\n",
            "                'learning_rate': {'distribution': 'uniform',\n",
            "                                  'max': 0.1,\n",
            "                                  'min': 0},\n",
            "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn44HMDPWPa3"
      },
      "source": [
        "# Step 2. Initialize the Sweep\n",
        "\n",
        "Weights and Biases has something called the Sweep Controller that handles the Sweep and issues a new set of instructions describing a new run to execute locally on our machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvyQHgXSPk5B",
        "outputId": "361c6ec8-32ac-40f0-f8b5-d725605c8916"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='pytorch-sweeps-demo')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 1an35jw5\n",
            "Sweep URL: https://wandb.ai/jacquesthibs/pytorch-sweeps-demo/sweeps/1an35jw5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WbyotW21KnH"
      },
      "source": [
        "# Step 3. Run the Sweep agent\n",
        "\n",
        "Before we can actually execute the sweep,\n",
        "we need to define the training procedure that uses those values.\n",
        "\n",
        "In the functions below, we define a simple fully-connected neural network in PyTorch, and add the following `wandb` tools to log model metrics, visualize performance and output and track our experiments:\n",
        "* [**`wandb.init()`**](https://docs.wandb.com/library/init) – Initialize a new W&B Run. Each Run is a single execution of the training function.\n",
        "* [**`wandb.config`**](https://docs.wandb.com/library/config) – Save all your hyperparameters in a configuration object so they can be logged. Read more about how to use `wandb.config` [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb).\n",
        "* [**`wandb.log()`**](https://docs.wandb.com/library/log) – log model behavior to W&B. Here, we just log the performance; see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb) for all the other rich media that can be logged with `wandb.log`.\n",
        "\n",
        "For more details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcq5Xqr4XJ4I"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "      # If called by wandb.agent, as below,\n",
        "      # this config will be set by Sweep Controller\n",
        "      config = wandb.config\n",
        "\n",
        "      loader = build_dataset(config.batch_size)\n",
        "      network = build_network(config.fc_layer_size, config.dropout)\n",
        "      optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "\n",
        "      for epoch in range(config.epochs):\n",
        "          avg_loss = train_epoch(network, loader, optimizer)\n",
        "          wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaUwtU_3N2p"
      },
      "source": [
        "This cell defines the four pieces of our training procedure:\n",
        "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`. These are all part of the standard PyTorch pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0aTmzVC280F"
      },
      "source": [
        "def build_dataset(batch_size): # input is in sweep config\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081))]\n",
        "    )\n",
        "    # download MNIST training dataset\n",
        "    dataset = datasets.MNIST('.', train=True, download=True,\n",
        "                            transform=transform)\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "        dataset, indices=range(0, len(dataset), 5)\n",
        "    )\n",
        "    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)\n",
        "\n",
        "    return loader\n",
        "\n",
        "def build_network(fc_layer_size, dropout): # inputs are in sweep config\n",
        "    network = nn.Sequential(\n",
        "        nn.Flatten(), # flatten image\n",
        "        nn.Linear(784, fc_layer_size), nn.ReLU(), # fully-connected layer with activation function\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(fc_layer_size, 10),\n",
        "        nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "    return network.to(device)\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optmizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "        \n",
        "    return optimizer\n",
        "\n",
        "def train_epoch(network, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    for _, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss = F.nll_loss(network(data), target)\n",
        "        cumu_loss += loss.item()\n",
        "\n",
        "        # Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item})\n",
        "\n",
        "    return cumu_loss / len(loader)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7dTNA99iT8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UmIyC6Y7NQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}