{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter-sweeps-using-wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyORfMXQICqkMV8nDHHzz0Lf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/effective-ml-mini-projects/blob/main/hyperparameter_sweeps_using_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-uLCK23-KIs"
      },
      "source": [
        "# How to do a Hyperparameter Sweep in Weights and Biases\n",
        "\n",
        "A hyperparameter sweep is used when we want to do hyperparameter tuning of our machine learning model. Weights and Biases allows uses to do sweeps fairly easily and creates some beautiful graphs to see how well our model did after training it with different sets of hyperparameters.\n",
        "\n",
        "# An Overview of Sweeps\n",
        "\n",
        "There a 3 steps to running a sweep with Weights and Biases:\n",
        "\n",
        "1. **Define the sweep:** create a dictionary or YAML file that specifies the parameters to search through, the search strategy, the optimization strategy, etc.\n",
        "\n",
        "2. **Initialize the sweep:** initialize the sweep and pass the dictionary of sweep configurations with: `sweep_id = wandb.sweep(sweep_config)`.\n",
        "\n",
        "3. **Run the sweep agent:** call `wandb.agent(sweep_id, function=train)`, where `function` defines the model architecture and trains it.\n",
        "\n",
        "If you decide the create a YAML file for your sweep, it should look something like this for a **deep learning** model:\n",
        "\n",
        "    # sweep.yaml\n",
        "    program: train.py\n",
        "    method: random\n",
        "    metric:\n",
        "     name: val_loss\n",
        "     goal: minimize\n",
        "    parameters:\n",
        "     learning-rate:\n",
        "       min: 0.00001\n",
        "       max: 0.1\n",
        "     optimizer:\n",
        "       values: [\"adam\", \"sgd\"]\n",
        "     hidden_layer_size:\n",
        "       values: [96, 128, 148]\n",
        "     epochs:\n",
        "       value: 27\n",
        "    early_terminate:\n",
        "       type: hyperband\n",
        "       s: 2\n",
        "       eta: 3\n",
        "       max_iter: 27\n",
        "\n",
        "And if you use a python dictionary, it should look like this for an xgboost model:\n",
        "\n",
        "    sweep_config = {\n",
        "        \"method\": \"random\", # try grid or random\n",
        "        \"metric\": {\n",
        "          \"name\": \"accuracy\",\n",
        "          \"goal\": \"maximize\"   \n",
        "        },\n",
        "        \"parameters\": {\n",
        "            \"booster\": {\n",
        "                \"values\": [\"gbtree\",\"gblinear\"]\n",
        "            },\n",
        "            \"max_depth\": {\n",
        "                \"values\": [3, 6, 9, 12]\n",
        "            },\n",
        "            \"learning_rate\": {\n",
        "                \"values\": [0.1, 0.05, 0.2]\n",
        "            },\n",
        "            \"subsample\": {\n",
        "                \"values\": [1, 0.5, 0.3]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "If you want to run a sweep from the command-line, you can run the following commands:\n",
        "\n",
        "1. **Setup a new sweep:** `wandb sweep sweep.yaml` which creates your sweep, and returns both a unique identifier (SWEEP_ID) and a URL to track all your runs.\n",
        "\n",
        "2. **Launch the sweep:** `wandb agent SWEEP_ID`, this will start the hyperparameter sweep and return the URL where you can track the sweep's progress. You can also launch multiple agents (GPUs / CPUs) concurrently. Each of these agents will fetch parameters from the W&B server and use them to train the next model.\n",
        "\n",
        "Documentation on sweeps can be found here: https://docs.wandb.ai/guides/sweeps/quickstart\n",
        "\n",
        "Now, let's get started with code!\n",
        "\n",
        "## Setup\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9tAzvxY6cKu"
      },
      "source": [
        "!pip install wandb --upgrade\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}